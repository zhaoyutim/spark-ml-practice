{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Spark ML\n",
    "In this lab assignment, you will complete a project by going through the following steps:\n",
    "1. Get the data.\n",
    "2. Discover the data to gain insights.\n",
    "3. Prepare the data for Machine Learning algorithms.\n",
    "4. Select a model and train it.\n",
    "5. Fine-tune your model.\n",
    "6. Present your solution.\n",
    "\n",
    "As a dataset, we use the California Housing Prices dataset from the StatLib repository. This dataset was based on data from the 1990 California census. The dataset has the following columns\n",
    "1. `longitude`: a measure of how far west a house is (a higher value is farther west)\n",
    "2. `latitude`: a measure of how far north a house is (a higher value is farther north)\n",
    "3. `housing_,median_age`: median age of a house within a block (a lower number is a newer building)\n",
    "4. `total_rooms`: total number of rooms within a block\n",
    "5. `total_bedrooms`: total number of bedrooms within a block\n",
    "6. `population`: total number of people residing within a block\n",
    "7. `households`: total number of households, a group of people residing within a home unit, for a block\n",
    "8. `median_income`: median income for households within a block of houses\n",
    "9. `median_house_value`: median house value for households within a block\n",
    "10. `ocean_proximity`: location of the house w.r.t ocean/sea\n",
    "\n",
    "---\n",
    "# 1. Get the data\n",
    "Let's start the lab by loading the dataset. The can find the dataset at `data/housing.csv`. To infer column types automatically, when you are reading the file, you need to set `inferSchema` to true. Moreover enable the `header` option to read the columns' name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 8 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val housing = spark.read.option(\"inferSchema\",true).option(\"header\",true).format(\"csv\").load(\"data/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Discover the data to gain insights\n",
    "Now it is time to take a look at the data. In this step we are going to take a look at the data a few different ways:\n",
    "* See the schema and dimension of the dataset\n",
    "* Look at the data itself\n",
    "* Statistical summary of the attributes\n",
    "* Breakdown of the data by the categorical attribute variable\n",
    "* Find the correlation among different attributes\n",
    "* Make new attributes by combining existing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Schema and dimension\n",
    "Print the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at the data\n",
    "Print the first five records of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>-122.23</td><td>37.88</td><td>41.0</td><td>880.0</td><td>129.0</td><td>322.0</td><td>126.0</td><td>8.3252</td><td>452600.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.22</td><td>37.86</td><td>21.0</td><td>7099.0</td><td>1106.0</td><td>2401.0</td><td>1138.0</td><td>8.3014</td><td>358500.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.24</td><td>37.85</td><td>52.0</td><td>1467.0</td><td>190.0</td><td>496.0</td><td>177.0</td><td>7.2574</td><td>352100.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>1274.0</td><td>235.0</td><td>558.0</td><td>219.0</td><td>5.6431</td><td>341300.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>1627.0</td><td>280.0</td><td>565.0</td><td>259.0</td><td>3.8462</td><td>342200.0</td><td>NEAR BAY</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+-------+------+--------+--------+--------+--------+--------+----------+----------+\n",
       "| -122.23 | 37.88 | 41.0 | 880.0  | 129.0  | 322.0  | 126.0  | 8.3252 | 452600.0 | NEAR BAY |\n",
       "| -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY |\n",
       "| -122.24 | 37.85 | 52.0 | 1467.0 | 190.0  | 496.0  | 177.0  | 7.2574 | 352100.0 | NEAR BAY |\n",
       "| -122.25 | 37.85 | 52.0 | 1274.0 | 235.0  | 558.0  | 219.0  | 5.6431 | 341300.0 | NEAR BAY |\n",
       "| -122.25 | 37.85 | 52.0 | 1627.0 | 280.0  | 565.0  | 259.0  | 3.8462 | 342200.0 | NEAR BAY |\n",
       "+---------+-------+------+--------+--------+--------+--------+--------+----------+----------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records with population more than 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -121.92|   37.53|               7.0|    28258.0|        3864.0|   12203.0|    3701.0|       8.4045|          451100.0|      <1H OCEAN|\n",
      "|  -117.78|   34.03|               8.0|    32054.0|        5290.0|   15507.0|    5050.0|       6.0191|          253900.0|      <1H OCEAN|\n",
      "|  -117.87|   34.04|               7.0|    27700.0|        4179.0|   15037.0|    4072.0|       6.6288|          339700.0|      <1H OCEAN|\n",
      "|  -117.88|   33.96|              16.0|    19059.0|        3079.0|   10988.0|    3061.0|       5.5469|          265200.0|      <1H OCEAN|\n",
      "|  -118.78|   34.16|               9.0|    30405.0|        4093.0|   12873.0|    3931.0|       8.0137|          399200.0|     NEAR OCEAN|\n",
      "|  -118.09|   34.68|               4.0|    23386.0|        4171.0|   10493.0|    3671.0|       4.0211|          144000.0|         INLAND|\n",
      "|   -118.1|   34.57|               7.0|    20377.0|        4335.0|   11973.0|    3933.0|       3.3086|          138100.0|         INLAND|\n",
      "|  -118.46|    34.4|              12.0|    25957.0|        4798.0|   10475.0|    4490.0|        4.542|          195300.0|      <1H OCEAN|\n",
      "|  -121.61|   36.69|              19.0|     9899.0|        2617.0|   11272.0|    2528.0|       2.0244|          118500.0|      <1H OCEAN|\n",
      "|  -121.68|   36.72|              12.0|    19234.0|        4492.0|   12153.0|    4372.0|       3.2652|          152800.0|      <1H OCEAN|\n",
      "|  -121.79|   36.64|              11.0|    32627.0|        6445.0|   28566.0|    6082.0|       2.3087|          118800.0|      <1H OCEAN|\n",
      "|  -117.74|   33.89|               4.0|    37937.0|        5471.0|   16122.0|    5189.0|       7.4947|          366300.0|      <1H OCEAN|\n",
      "|  -117.12|   33.52|               4.0|    30401.0|        4957.0|   13251.0|    4339.0|       4.5841|          212300.0|      <1H OCEAN|\n",
      "|  -121.53|   38.48|               5.0|    27870.0|        5027.0|   11935.0|    4855.0|       4.8811|          212200.0|         INLAND|\n",
      "|   -121.4|   38.47|               4.0|    20982.0|        3392.0|   10329.0|    3086.0|       4.3658|          130600.0|         INLAND|\n",
      "|  -121.44|   38.43|               3.0|    39320.0|        6210.0|   16305.0|    5358.0|       4.9516|          153700.0|         INLAND|\n",
      "|  -117.75|   34.01|               4.0|    22128.0|        3522.0|   10450.0|    3258.0|       6.1287|          289600.0|      <1H OCEAN|\n",
      "|  -117.61|    34.1|               9.0|    18956.0|        4095.0|   10323.0|    3832.0|       3.6033|          132600.0|         INLAND|\n",
      "|  -116.14|   34.45|              12.0|     8796.0|        1721.0|   11139.0|    1680.0|       2.2612|          137500.0|         INLAND|\n",
      "|  -117.42|   33.35|              14.0|    25135.0|        4819.0|   35682.0|    4769.0|       2.5729|          134400.0|      <1H OCEAN|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.filter(\"population > 10000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Statistical summary\n",
    "Print a summary of the table statistics for the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. You can use the `describe` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|housing_median_age|       total_rooms|median_house_value|        population|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|\n",
      "|   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n",
      "| stddev| 12.58555761211163|2181.6152515827944|115395.61587441359|  1132.46212176534|\n",
      "|    min|               1.0|               2.0|           14999.0|               3.0|\n",
      "|    max|              52.0|           39320.0|          500001.0|           35682.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.describe(\"housing_median_age\",\"total_rooms\",\"median_house_value\",\"population\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the maximum age (`housing_median_age`), the minimum number of rooms (`total_rooms`), and the average of house values (`median_house_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------+-----------------------+\n",
      "|max(housing_median_age)|min(total_rooms)|avg(median_house_value)|\n",
      "+-----------------------+----------------+-----------------------+\n",
      "|                   52.0|             2.0|     206855.81690891474|\n",
      "+-----------------------+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "housing.select(max(\"housing_median_age\"),min(\"total_rooms\"),avg(\"median_house_value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Breakdown the data by categorical data\n",
    "Print the number of houses in different areas (`ocean_proximity`), and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|ocean_proximity|count(households)|\n",
      "+---------------+-----------------+\n",
      "|      <1H OCEAN|             9136|\n",
      "|         INLAND|             6551|\n",
      "|     NEAR OCEAN|             2658|\n",
      "|       NEAR BAY|             2290|\n",
      "|         ISLAND|                5|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.groupBy(\"ocean_proximity\").agg(count(\"households\")).orderBy(desc(\"count(households)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average value of the houses (`median_house_value`) in different areas (`ocean_proximity`), and call the new column `avg_value` when print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.groupBy(\"ocean_proximity\").agg(avg(\"median_house_value\")).withColumnRenamed(\"avg(median_house_value)\", \"avg_value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above question in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"select ocean_proximity, avg(median_house_value) as avg_value from df group by ocean_proximity\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Correlation among attributes\n",
    "Print the correlation among the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. To do so, first you need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, you can use the `VectorAssembler` Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|          attributes|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_8e0af4e0c709\n",
       "housingAttrs = [longitude: double, latitude: double ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 9 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"housing_median_age\",\"total_rooms\",\"median_house_value\",\"population\")).setOutputCol(\"attributes\")\n",
    "\n",
    "val housingAttrs = va.transform(housing)\n",
    "\n",
    "housingAttrs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard correlation coefficient:\n",
      " 1.0                   -0.36126220122231784  0.10562341249318154   -0.2962442397735293   \n",
      "-0.36126220122231784  1.0                   0.13415311380654338   0.8571259728659772    \n",
      "0.10562341249318154   0.13415311380654338   1.0                   -0.02464967888891235  \n",
      "-0.2962442397735293   0.8571259728659772    -0.02464967888891235  1.0                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "coeff = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0                   -0.36126220122231784  0.10562341249318154   -0.2962442397735293   \n",
       "-0.36126220122231784  1.0                   0.13415311380654338   0.8571259728659772    \n",
       "0.10562341249318154   0.13415311380654338   1.0                   -0.02464967888891235  \n",
       "-0.2962442397735293   0.8571259728659772    -0.02464967888891235  1.0                   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.linalg.Matrix\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val Row(coeff: Matrix) = Correlation.corr(housingAttrs,\"attributes\").head\n",
    "\n",
    "println(s\"The standard correlation coefficient:\\n ${coeff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Combine and make new attributes\n",
    "Now, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful, if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, add the three new columns to the dataset as below. We will call the new dataset the `housingExtra`.\n",
    "```\n",
    "rooms_per_household = total_rooms / households\n",
    "bedrooms_per_room = total_bedrooms / total_rooms\n",
    "population_per_household = population / households\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------+\n",
      "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+-------------------+-------------------+------------------------+\n",
      "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "housingCol1 = [longitude: double, latitude: double ... 9 more fields]\n",
       "housingCol2 = [longitude: double, latitude: double ... 10 more fields]\n",
       "housingExtra = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 11 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val housingCol1 = housing.withColumn(\"rooms_per_household\",housing(\"total_rooms\")/housing(\"households\"))\n",
    "val housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\",housing(\"total_bedrooms\") / housing(\"total_rooms\"))\n",
    "val housingExtra = housingCol2.withColumn(\"population_per_household\",housing(\"population\") / housing(\"households\"))\n",
    "\n",
    "housingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prepare the data for Machine Learning algorithms\n",
    "Before going through the Machine Learning steps, let's first rename the label column from `median_house_value` to `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renamedHousing = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 11 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val renamedHousing = housingExtra.withColumnRenamed(\"median_house_value\",\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to separate the numerical attributes from the categorical attribute (`ocean_proximity`) and keep their column names in two different lists. Moreover, sice we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel = label\n",
       "colCat = ocean_proximity\n",
       "colNum = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// label columns\n",
    "val colLabel = \"label\"\n",
    "\n",
    "// categorical columns\n",
    "val colCat = \"ocean_proximity\"\n",
    "\n",
    "// numerical columns\n",
    "val colNum = renamedHousing.columns.filter(_ != colLabel).filter(_ != colCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepare continuse attributes\n",
    "### Data cleaning\n",
    "Most Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continues attributes, listed in `colNum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|sum(CASE WHEN (longitude IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+--------------------------------------------------------+\n",
      "|                                                       0|\n",
      "+--------------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------------+\n",
      "|sum(CASE WHEN (latitude IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+-------------------------------------------------------+\n",
      "|                                                      0|\n",
      "+-------------------------------------------------------+\n",
      "\n",
      "+-----------------------------------------------------------------+\n",
      "|sum(CASE WHEN (housing_median_age IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+-----------------------------------------------------------------+\n",
      "|                                                                0|\n",
      "+-----------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------+\n",
      "|sum(CASE WHEN (total_rooms IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+----------------------------------------------------------+\n",
      "|                                                         0|\n",
      "+----------------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------------------+\n",
      "|sum(CASE WHEN (total_bedrooms IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+-------------------------------------------------------------+\n",
      "|                                                          207|\n",
      "+-------------------------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|sum(CASE WHEN (population IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+---------------------------------------------------------+\n",
      "|                                                        0|\n",
      "+---------------------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|sum(CASE WHEN (households IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+---------------------------------------------------------+\n",
      "|                                                        0|\n",
      "+---------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------+\n",
      "|sum(CASE WHEN (median_income IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+------------------------------------------------------------+\n",
      "|                                                           0|\n",
      "+------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------+\n",
      "|sum(CASE WHEN (rooms_per_household IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+------------------------------------------------------------------+\n",
      "|                                                                 0|\n",
      "+------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------+\n",
      "|sum(CASE WHEN (bedrooms_per_room IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+----------------------------------------------------------------+\n",
      "|                                                             207|\n",
      "+----------------------------------------------------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------+\n",
      "|sum(CASE WHEN (population_per_household IS NOT NULL) THEN 0 ELSE 1 END)|\n",
      "+-----------------------------------------------------------------------+\n",
      "|                                                                      0|\n",
      "+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "for (c <- colNum) {\n",
    "    renamedHousing.select(c).agg(sum(when(col(c).isNotNull,0).otherwise(1))).show()\n",
    "    //renamedHousing.groupBy(c).agg(count(\"*\").alias(\"numOfNull\")).where(col(c).isNull).show()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observerd above, the `total_bedrooms` and `bedrooms_per_room` attributes have some missing values. One way to take care of missing values is to use the `Imputer` Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an `Imputer` instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|total_bedrooms|  bedrooms_per_room|\n",
      "+--------------+-------------------+\n",
      "|         129.0|0.14659090909090908|\n",
      "|        1106.0|0.15579659106916466|\n",
      "|         190.0|0.12951601908657123|\n",
      "|         235.0|0.18445839874411302|\n",
      "|         280.0| 0.1720958819913952|\n",
      "+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "imputer = imputer_cfd2b3eb0c01\n",
       "imputedHousing = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 11 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer().setStrategy(\"mean\").setInputCols(Array(\"total_bedrooms\",\"bedrooms_per_room\")).setOutputCols(Array(\"total_bedrooms\",\"bedrooms_per_room\"))                                  \n",
    "val imputedHousing = imputer.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "imputedHousing.select(\"total_bedrooms\", \"bedrooms_per_room\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attribues is generally not required.\n",
    "\n",
    "One way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. To do this, we can use the `StandardScaler` Estimator. To use `StandardScaler`, again we need to convert all the numerical attributes into a big vectore of features using `VectorAssembler`, and then call `StandardScaler` on that vactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|   numericalFeatures|      scaledFeatures|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_1ca89bb51a73\n",
       "featuredHousing = [longitude: double, latitude: double ... 12 more fields]\n",
       "scaler = stdScal_97390b9bc023\n",
       "scaledHousing = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 13 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(colNum).setOutputCol(\"numericalFeatures\")\n",
    "val featuredHousing = va.transform(imputedHousing)\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"numericalFeatures\").setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)\n",
    "val scaledHousing = scaler.fit(featuredHousing).transform(featuredHousing)\n",
    "\n",
    "scaledHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Prepare categorical attributes\n",
    "After imputing and scaling the continuse attributes, we should take care of the categorical attributes. Let's first print the number of distict values of the categirical attribute `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|ocean_proximity|\n",
      "+---------------+\n",
      "|         ISLAND|\n",
      "|     NEAR OCEAN|\n",
      "|       NEAR BAY|\n",
      "|      <1H OCEAN|\n",
      "|         INLAND|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "renamedHousing.select(\"ocean_proximity\").distinct.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String indexer\n",
    "Most Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute `ocean_proximity` to numbers. To do so, we can use the `StringIndexer` that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|indexed_ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer = strIdx_ecbe136af6fc\n",
       "idxHousing = [longitude: double, latitude: double ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 12 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"ocean_proximity\").setOutputCol(\"indexed_ocean_proximity\")\n",
    "val idxHousing = indexer.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "idxHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this numerical data in any Machine Learning algorithm. You can look at the mapping that this encoder has learned using the `labels` method: \"<1H OCEAN\" is mapped to 0, \"INLAND\" is mapped to 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.fit(renamedHousing).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "Now, convert the label indices built in the last step into one-hot vectors. To do this, you can take advantage of the `OneHotEncoderEstimator` Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|indexed_ocean_proximity|encoded_ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|          (4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder = oneHotEncoder_b08f4a588f38\n",
       "ohHousing = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 13 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator().setInputCols(Array(\"indexed_ocean_proximity\")).setOutputCols(Array(\"encoded_ocean_proximity\"))\n",
    "val ohHousing = encoder.fit(idxHousing).transform(idxHousing)\n",
    "\n",
    "ohHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Pipeline\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. For example, you called the `Imputer`, `VectorAssembler`, and `StandardScaler` from left to right. However, we can use the `Pipeline` class to define a sequence of Transformers/Estimators, and run them in order. A `Pipeline` is an `Estimator`, thus, after a Pipeline's `fit()` method runs, it produces a `PipelineModel`, which is a `Transformer`.\n",
    "\n",
    "Now, let's create a pipeline called `numPipeline` to call the numerical transformers you built above (`imputer`, `va`, and `scaler`) in the right order from left to right, as well as a pipeline called `catPipeline` to call the categorical transformers (`indexer` and `encoder`). Then, put these two pipelines `numPipeline` and `catPipeline` into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|   numericalFeatures|      scaledFeatures|indexed_ocean_proximity|encoded_ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|                    3.0|          (4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|                    3.0|          (4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numPipeline = pipeline_eb72216c1cfc\n",
       "catPipeline = pipeline_2f6b9bf3a724\n",
       "pipeline = pipeline_373e3d1df1f6\n",
       "newHousing = [longitude: double, latitude: double ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 15 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
    "\n",
    "val numPipeline = new Pipeline().setStages(Array(imputer,va,scaler))\n",
    "val catPipeline = new Pipeline().setStages(Array(indexer,encoder))\n",
    "val pipeline = new Pipeline().setStages(Array(numPipeline, catPipeline))\n",
    "val newHousing = pipeline.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "newHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `VectorAssembler` to put all attributes of the final dataset `newHousing` into a big vector, and call the new column `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-61.007269596069...|452600.0|\n",
      "|[-61.002278409814...|358500.0|\n",
      "|[-61.012260782324...|352100.0|\n",
      "|[-61.017251968579...|341300.0|\n",
      "|[-61.017251968579...|342200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va2 = vecAssembler_8f978c6633ce\n",
       "dataset = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val va2 = new VectorAssembler().setInputCols(Array(\"scaledFeatures\",\"encoded_ocean_proximity\")).setOutputCol(\"features\")\n",
    "val dataset = va2.transform(newHousing).select(\"features\", \"label\")\n",
    "\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Make a model\n",
    "Here we going to make four different regression models:\n",
    "* Linear regression model\n",
    "* Decission tree regression\n",
    "* Random forest regression\n",
    "* Gradient-booster forest regression\n",
    "\n",
    "But, before giving the data to train a Machine Learning model, let's first split the data into training dataset (`trainSet`) with 80% of the whole data, and test dataset (`testSet`) with 20% of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSet = [features: vector, label: double]\n",
       "testSet = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Linear regression model\n",
    "Now, train a Linear Regression model using the `LinearRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-15556.506397732895,-12473.634154038255,15113.813744926687,8533.494106827868,13039.230784818687,-29805.91454292005,14083.1643892406,83530.39545687684,3943.7421796881536,19445.065502057052,-1050.0494313024044,18186.551818566506,-37627.691640125886,31270.937454777064,18379.168006061882], Intercept: -808157.4971628699\n",
      "RMSE: 68664.97716947603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = linReg_39eeaf8d63cd\n",
       "lrModel = linReg_39eeaf8d63cd\n",
       "trainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@6ceecf92\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.regression.LinearRegressionTrainingSummary@6ceecf92"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// train the model\n",
    "val lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(trainSet)\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"Coefficients: ${lrModel.coefficients}, Intercept: ${lrModel.intercept}\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `RegressionEvaluator` to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "| 147259.7645971781|66900.0|[-61.995524474579...|\n",
      "|164561.64518864825|68400.0|[-61.995524474579...|\n",
      "|168478.51836781914|70000.0|[-61.985542102068...|\n",
      "| 173984.0086774386|67000.0|[-61.980550915813...|\n",
      "| 148994.2908594812|64600.0|[-61.980550915813...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 70368.95634488532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_fdc89edb8a91\n",
       "rmse = 70368.95634488532\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "70368.95634488532"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Decision tree regression\n",
    "Repeat what you have done on Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|138419.96238244514|66900.0|[-61.995524474579...|\n",
      "|167078.95072308517|68400.0|[-61.995524474579...|\n",
      "|146292.13857677902|70000.0|[-61.985542102068...|\n",
      "|138419.96238244514|67000.0|[-61.980550915813...|\n",
      "|138419.96238244514|64600.0|[-61.980550915813...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 69589.84468293576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt = dtr_6b1f3dabdb3a\n",
       "dtModel = DecisionTreeRegressionModel (uid=dtr_6b1f3dabdb3a) of depth 5 with 63 nodes\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_dcd3ff4fd38c\n",
       "rmse = 69589.84468293576\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "69589.84468293576"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val dt = new DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val dtModel = dt.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = dtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Random forest regression\n",
    "Let's try the test error on a Random Forest Model. Youcan use the `RandomForestRegressor` to make a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "| 166879.5319449747|66900.0|[-61.995524474579...|\n",
      "|  171013.009593966|68400.0|[-61.995524474579...|\n",
      "|173003.11859628579|70000.0|[-61.985542102068...|\n",
      "|160469.74414201372|67000.0|[-61.980550915813...|\n",
      "|160469.74414201372|64600.0|[-61.980550915813...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 67366.37099345516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf = rfr_720cb0af7a3a\n",
       "rfModel = RandomForestRegressionModel (uid=rfr_720cb0af7a3a) with 10 trees\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_8f5e0261f62d\n",
       "rmse = 67366.37099345516\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "67366.37099345516"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val rf = new RandomForestRegressor().setNumTrees(10)\n",
    "\n",
    "// train the model\n",
    "val rfModel = rf.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = rfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Gradient-boosted tree regression\n",
    "Fianlly, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. Use the `GBTRegressor` to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  label|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|108889.41761252526|66900.0|[-61.995524474579...|\n",
      "|144577.81726789603|68400.0|[-61.995524474579...|\n",
      "| 84829.13112772178|70000.0|[-61.985542102068...|\n",
      "|104290.17153440078|67000.0|[-61.980550915813...|\n",
      "|  97491.8478314796|64600.0|[-61.980550915813...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 60653.91854580034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gb = gbtr_f630f363c76a\n",
       "gbModel = GBTRegressionModel (uid=gbtr_f630f363c76a) with 10 trees\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_155f5069b0e8\n",
       "rmse = 60653.91854580034\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "60653.91854580034"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val gb = new GBTRegressor().setMaxIter(10).setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "// train the model\n",
    "val gbModel = gb.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = gbModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: all, current: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name (default: label)\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute (default: squared)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 20, current: 10)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: -131597770)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "validationIndicatorCol: name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation. (undefined)\n",
      "validationTol: Threshold for stopping early when fit with validation is used.If the error rate on the validation input changes by less than the validationTol,then learning will stop early (before `maxIter`).This parameter is ignored when fit without validation is used. (default: 0.01)\n"
     ]
    }
   ],
   "source": [
    "println(gb.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Hyperparameter tuning\n",
    "An important task in Machie Learning is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as `CrossValidator`. These tools require the following items:\n",
    "* Estimator: algorithm or Pipeline to tune (`setEstimator`)\n",
    "* Set of ParamMaps: parameters to choose from, sometimes called a \"parameter grid\" to search over (`setEstimatorParamMaps`)\n",
    "* Evaluator: metric to measure how well a fitted Model does on held-out test data (`setEvaluator`)\n",
    "\n",
    "`CrossValidator` begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example with `k=3` folds, `CrossValidator` will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular `ParamMap`, `CrossValidator` computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs. After identifying the best `ParamMap`, `CrossValidator` finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "Below, use the `CrossValidator` to select the best Random Forest model. To do so, you need to define a grid of parameters. Let's say we want to do the search among the different number of trees (1, 5, and 10), and different tree depth (5, 10, and 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+--------------------+\n",
      "|       prediction|  label|            features|\n",
      "+-----------------+-------+--------------------+\n",
      "|         109345.0|66900.0|[-61.995524474579...|\n",
      "|         137495.0|68400.0|[-61.995524474579...|\n",
      "|79485.14705882352|70000.0|[-61.985542102068...|\n",
      "|79088.76427494074|67000.0|[-61.980550915813...|\n",
      "|75913.45475113123|64600.0|[-61.980550915813...|\n",
      "+-----------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 53476.83867334043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\trfr_720cb0af7a3a-maxDepth: 5,\n",
       "\trfr_720cb0af7a3a-numTrees: 1\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 5,\n",
       "\trfr_720cb0af7a3a-numTrees: 5\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 5,\n",
       "\trfr_720cb0af7a3a-numTrees: 10\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 10,\n",
       "\trfr_720cb0af7a3a-numTrees: 1\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 10,\n",
       "\trfr_720cb0af7a3a-numTrees: 5\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 10,\n",
       "\trfr_720cb0af7a3a-numTrees: 10\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 15,\n",
       "\trfr_720cb0af7a3a-numTrees: 1\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 15,\n",
       "\trfr_720cb0af7a3a-numTrees: 5\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 15,\n",
       "\trfr_720c...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\trfr_720cb0af7a3a-maxDepth: 5,\n",
       "\trfr_720cb0af7a3a-numTrees: 1\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 5,\n",
       "\trfr_720cb0af7a3a-numTrees: 5\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 5,\n",
       "\trfr_720cb0af7a3a-numTrees: 10\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 10,\n",
       "\trfr_720cb0af7a3a-numTrees: 1\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 10,\n",
       "\trfr_720cb0af7a3a-numTrees: 5\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 10,\n",
       "\trfr_720cb0af7a3a-numTrees: 10\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 15,\n",
       "\trfr_720cb0af7a3a-numTrees: 1\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 15,\n",
       "\trfr_720cb0af7a3a-numTrees: 5\n",
       "}, {\n",
       "\trfr_720cb0af7a3a-maxDepth: 15,\n",
       "\trfr_720cb0af7a3a-numTrees: 10\n",
       "}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.numTrees,Array(1,5,10)).addGrid(rf.maxDepth,Array(5,10,15)).build()\n",
    "\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    "val cv = new CrossValidator().setEstimator(rf).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainSet)\n",
    "\n",
    "val predictions = cvModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Custom transformer\n",
    "At the end of part two, we added extra columns to the `housing` dataset. Here, we are going to implement a Transformer to do the same task. The Transformer should take the name of two input columns `inputCol1` and `inputCol2`, as well as the name of ouput column `outputCol`. It, then, computes `inputCol1` divided by `inputCol2`, and adds its result as a new column to the dataset. The details of the implemeting a custom Tranfomer is explained [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types). Please read it before before starting to implement it.\n",
    "\n",
    "First, define the given parameters of the Transformer and implement a method to validate their schemas (`StructType`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait MyParams\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, DoubleType}\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "\n",
    "trait MyParams extends Params {\n",
    "    final val inputCol1 = new Param[String](this, \"inputCol1\", \"The input column1\")\n",
    "    final val inputCol2 = new Param[String](this, \"inputCol2\", \"The input column2\")\n",
    "    final val outputCol = new Param[String](this, \"outputCol\", \"The output column\")\n",
    "    \n",
    "  protected def validateAndTransformSchema(schema: StructType): StructType = {\n",
    "    // Check that the input type is a string\n",
    "    val idx1 = schema.fieldIndex($(inputCol1))\n",
    "    val field1 = schema.fields(idx1)\n",
    "    if (field1.dataType != DoubleType) {\n",
    "      throw new Exception(\"Input type ${field1.dataType} did not match input type DoubleType\")\n",
    "    }\n",
    "    val idx2 = schema.fieldIndex($(inputCol2))\n",
    "    val field2 = schema.fields(idx2)\n",
    "    if (field2.dataType != DoubleType) {\n",
    "      throw new Exception(\"Input type ${field2.dataType} did not match input type DoubleType\")\n",
    "    }\n",
    "    // Add the return field\n",
    "    schema.add(StructField($(outputCol), DoubleType, false))\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extend the class `Transformer`, and implement its setter functions for the input and output columns, and call then `setInputCol1`, `setInputCol2`, and `setOutputCol`. Morever, you need to override the methods `copy`, `transformSchema`, and the `transform`. The details of what you need to cover in these methods is given [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MyTransformer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.util.Identifiable\n",
    "import org.apache.spark.ml.Transformer\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "\n",
    "class MyTransformer(override val uid: String) extends Transformer with MyParams {\n",
    "    def this() = this(Identifiable.randomUID(\"configurablewordcount\"))\n",
    "    \n",
    "    def setInputCol1(value: String) = set(inputCol1, value)\n",
    "    \n",
    "    def setInputCol2(value: String) = set(inputCol2, value)\n",
    "    \n",
    "    def setOutputCol(value: String) = set(outputCol, value)\n",
    "\n",
    "    override def copy(extra: ParamMap) : MyTransformer = { \n",
    "        defaultCopy(extra)\n",
    "    }\n",
    "    \n",
    "    override def transformSchema(schema: StructType):StructType={\n",
    "        validateAndTransformSchema(schema)\n",
    "    }\n",
    "    \n",
    "    override def transform(dataset: Dataset[_]): DataFrame = {\n",
    "        //val housingCol1 = housing.withColumn(\"rooms_per_household\", $\"total_rooms\" / $\"households\") \n",
    "        //val housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\", $\"total_bedrooms\" / $\"total_rooms\") \n",
    "        //val housingExtra = housingCol2.withColumn(\"population_per_household\", $\"population\" / $\"households\") \n",
    "        dataset.withColumn($(outputCol), dataset($(inputCol1)) / dataset($(inputCol2)))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, an instance of `MyTransformer`, and set the input columns `total_rooms` and `households`, and the output column `rooms_per_household` and run it over the `housing` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|rooms_per_household|\n",
      "+-------------------+\n",
      "|  6.984126984126984|\n",
      "|  6.238137082601054|\n",
      "|  8.288135593220339|\n",
      "| 5.8173515981735155|\n",
      "|  6.281853281853282|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myTransformer = configurablewordcount_2663b2044f89\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "myDataset: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "configurablewordcount_2663b2044f89"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myTransformer = new MyTransformer().setInputCol1(\"total_rooms\").setInputCol2(\"households\").setOutputCol(\"rooms_per_household\")\n",
    "\n",
    "val myDataset = myTransformer.transform(housing).select(\"rooms_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Custom estimator (predictor)\n",
    "Now, it's time to implement your own linear regression with gradient descent algorithm as a brand new Estimator. The whole code of the Estimator is given to you, and you do not need to implement anything. It is just a sample that shows how to build a custom Estimator.\n",
    "\n",
    "The gradient descent update for linear regression is:\n",
    "$$\n",
    "w_{i+1} = w_{i} - \\alpha_{i} \\sum\\limits_{j=1}^n (w_i^\\top x_j - y_j)x_j\n",
    "$$\n",
    "\n",
    "where $i$ is the iteration number of the gradient descent algorithm, and $j$ identifies the observation. Here, $w$ represents an array of weights that is the same size as the array of features and provides a weight for each of the features when finally computing the label prediction in the form:\n",
    "\n",
    "$$\n",
    "prediction = w^\\top \\cdot\\ x\n",
    "$$\n",
    "\n",
    "where $w$ is the final array of weights computed by the gradient descent, $x$ is the array of features of the observation point and $prediction$ is the label we predict should be associated to this observation.\n",
    "\n",
    "The given `Helper` class implements the helper methods:\n",
    "* `dot`: implements the dot product of two vectors and the dot product of a vector and a scalar\n",
    "* `sum`: implements addition of two vectors\n",
    "* `fill`: creates a vector of predefined size and initialize it with the predefined value\n",
    "\n",
    "What you need to do is to implement the methods of the Linear Regresstion class `LR`, which are\n",
    "* `rmsd`: computes the Root Mean Square Error of a given RDD of tuples of (label, prediction) using the formula:\n",
    "$$\n",
    "rmse = \\sqrt{\\frac{\\sum\\limits_{i=1}^n (label - prediction)^2}{n}}\n",
    "$$\n",
    "* `gradientSummand`: computes the following formula:\n",
    "$$\n",
    "gs_{ij} = (w_i^\\top x_j - y_j)x_j\n",
    "$$\n",
    "* `gradient`: computes the following formula:\n",
    "$$\n",
    "gradient = \\sum\\limits_{j=1}^n gs_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Instance\n",
       "defined object Helper\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictorParams\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.util._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.{PredictionModel, Predictor}\n",
    "\n",
    "case class Instance(label: Double, features: Vector)\n",
    "\n",
    "object Helper extends Serializable {\n",
    "  def dot(v1: Vector, v2: Vector): Double = {\n",
    "    val m = Matrices.dense(1, v1.size, v1.toArray)\n",
    "    m.multiply(v2).values(0)\n",
    "  }\n",
    "\n",
    "  def dot(v: Vector, s: Double): Vector = {\n",
    "    val baseArray = v.toArray.map(vi => vi * s)\n",
    "    Vectors.dense(baseArray)\n",
    "  }\n",
    "\n",
    "  def sumVectors(v1: Vector, v2: Vector): Vector = {\n",
    "    val baseArray = ((v1.toArray) zip (v2.toArray)).map { case (val1, val2) => val1 + val2 }\n",
    "    Vectors.dense(baseArray)\n",
    "  }\n",
    "\n",
    "  def fillVector(size: Int, fillVal: Double): Vector = Vectors.dense(Array.fill[Double](size)(fillVal));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:18: error: not found: type RDD\n",
       "         def calcRMSE(labelsAndPreds: RDD[(Double, Double)]): Double = {\n",
       "                                      ^\n",
       "<console>:19: error: not found: type RegressionMetrics\n",
       "           val regressionMetrics = new RegressionMetrics(labelsAndPreds)\n",
       "                                       ^\n",
       "<console>:23: error: type Vector takes type parameters\n",
       "         def gradientSummand(weights: Vector, lp: Instance): Vector = {\n",
       "                                                             ^\n",
       "<console>:23: error: type Vector takes type parameters\n",
       "         def gradientSummand(weights: Vector, lp: Instance): Vector = {\n",
       "                                      ^\n",
       "<console>:26: error: not found: value Vectors\n",
       "           return Vectors.dense(seq.toArray)\n",
       "                  ^\n",
       "<console>:29: error: type Vector takes type parameters\n",
       "         def linregGradientDescent(trainData: RDD[Instance], numIters: Int): (Vector, Array[Double]) = {\n",
       "                                                                              ^\n",
       "<console>:29: error: not found: type RDD\n",
       "         def linregGradientDescent(trainData: RDD[Instance], numIters: Int): (Vector, Array[Double]) = {\n",
       "                                              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LR() extends Serializable {\n",
    "  def calcRMSE(labelsAndPreds: RDD[(Double, Double)]): Double = {\n",
    "    val regressionMetrics = new RegressionMetrics(labelsAndPreds)\n",
    "    regressionMetrics.rootMeanSquaredError\n",
    "  }\n",
    "  \n",
    "  def gradientSummand(weights: Vector, lp: Instance): Vector = {\n",
    "    val mult = (Helper.dot(weights, lp.features) - lp.label)\n",
    "    val seq = (0 to lp.features.size - 1).map(i => lp.features(i) * mult)\n",
    "    return Vectors.dense(seq.toArray)\n",
    "  }\n",
    "  \n",
    "  def linregGradientDescent(trainData: RDD[Instance], numIters: Int): (Vector, Array[Double]) = {\n",
    "    val n = trainData.count()\n",
    "    val d = trainData.take(1)(0).features.size\n",
    "    var w = Helper.fillVector(d, 0)\n",
    "    val alpha = 1.0\n",
    "    val errorTrain = Array.fill[Double](numIters)(0.0)\n",
    "\n",
    "    for (i <- 0 until numIters) {\n",
    "      val labelsAndPredsTrain = trainData.map(lp => (lp.label, Helper.dot(w, lp.features)))\n",
    "      errorTrain(i) = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "      val gradient = trainData.map(lp => gradientSummand(w, lp)).reduce((v1, v2) => Helper.sumVectors(v1, v2))\n",
    "      val alpha_i = alpha / (n * scala.math.sqrt(i + 1))\n",
    "      val wAux = Helper.dot(gradient, (-1) * alpha_i)\n",
    "      w = Helper.sumVectors(w, wAux)\n",
    "    }\n",
    "    (w, errorTrain)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:16: error: not found: type PredictionModel\n",
       "         extends PredictionModel[FeaturesType, Model] {\n",
       "                 ^\n",
       "<console>:20: error: type Vector takes type parameters\n",
       "           extends MyLinearModel[Vector, MyLinearModelImpl] {\n",
       "                                 ^\n",
       "<console>:19: error: type Vector takes type parameters\n",
       "       class MyLinearModelImpl(override val uid: String, val weights: Vector, val trainingError: Array[Double])\n",
       "                                                                      ^\n",
       "<console>:22: error: not found: type ParamMap\n",
       "         override def copy(extra: ParamMap): MyLinearModelImpl = defaultCopy(extra)\n",
       "                                  ^\n",
       "<console>:22: error: not found: value defaultCopy\n",
       "         override def copy(extra: ParamMap): MyLinearModelImpl = defaultCopy(extra)\n",
       "                                                                 ^\n",
       "<console>:24: error: type Vector takes type parameters\n",
       "         def predict(features: Vector): Double = {\n",
       "                               ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract class MyLinearModel[FeaturesType, Model <: MyLinearModel[FeaturesType, Model]]\n",
    "  extends PredictionModel[FeaturesType, Model] {\n",
    "}\n",
    "\n",
    "class MyLinearModelImpl(override val uid: String, val weights: Vector, val trainingError: Array[Double])\n",
    "    extends MyLinearModel[Vector, MyLinearModelImpl] {\n",
    "\n",
    "  override def copy(extra: ParamMap): MyLinearModelImpl = defaultCopy(extra)\n",
    "\n",
    "  def predict(features: Vector): Double = {\n",
    "    println(\"Predicting\")\n",
    "    val prediction = Helper.dot(weights, features)\n",
    "    prediction\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:34: error: not found: type MyLinearModel\n",
       "           Model <: MyLinearModel[FeaturesType, Model]]\n",
       "                    ^\n",
       "<console>:39: error: not found: type MyLinearModelImpl\n",
       "           extends MyLinearRegression[Vector, MyLinearRegressionImpl, MyLinearModelImpl] {\n",
       "                                                                      ^\n",
       "<console>:44: error: not found: type MyLinearModelImpl\n",
       "         def train(dataset: Dataset[_]): MyLinearModelImpl = {\n",
       "                                         ^\n",
       "<console>:55: error: not found: type LR\n",
       "           val (weights, trainingError) = new LR().linregGradientDescent(instances, numIters)\n",
       "                                              ^\n",
       "<console>:57: error: not found: type MyLinearModelImpl\n",
       "           new MyLinearModelImpl(uid, weights, trainingError)\n",
       "               ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictorParams\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.util._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.{PredictionModel, Predictor}\n",
    "\n",
    "abstract class MyLinearRegression[\n",
    "    FeaturesType,\n",
    "    Learner <: MyLinearRegression[FeaturesType, Learner, Model],\n",
    "    Model <: MyLinearModel[FeaturesType, Model]]\n",
    "  extends Predictor[FeaturesType, Learner, Model] {\n",
    "}\n",
    "\n",
    "class MyLinearRegressionImpl(override val uid: String)\n",
    "    extends MyLinearRegression[Vector, MyLinearRegressionImpl, MyLinearModelImpl] {\n",
    "  def this() = this(Identifiable.randomUID(\"linReg\"))\n",
    "\n",
    "  override def copy(extra: ParamMap): MyLinearRegressionImpl = defaultCopy(extra)\n",
    "  \n",
    "  def train(dataset: Dataset[_]): MyLinearModelImpl = {\n",
    "    println(\"Training\")\n",
    "\n",
    "    val numIters = 10\n",
    "\n",
    "    val instances: RDD[Instance] = dataset.select(\n",
    "      col($(labelCol)), col($(featuresCol))).rdd.map {\n",
    "        case Row(label: Double, features: Vector) =>\n",
    "          Instance(label, features)\n",
    "      }\n",
    "\n",
    "    val (weights, trainingError) = new LR().linregGradientDescent(instances, numIters)\n",
    "\n",
    "    new MyLinearModelImpl(uid, weights, trainingError)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:82: error: not found: type MyLinearRegressionImpl\n",
       "       val lr = new MyLinearRegressionImpl().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val lr = new MyLinearRegressionImpl().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "val model = lr.fit(trainSet)\n",
    "val predictions = model.transform(trainSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. An End-to-End Classification Test\n",
    "As the last step, you are given a dataset called `data/ccdefault.csv`. The dataset represents default of credit card clients. It has 30,000 cases and 24 different attributes. More details about the dataset is available at `data/ccdefault.txt`. In this task you should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n",
    "1. Load the data.\n",
    "2. Carry out some exploratory analyses (e.g., how various features and the target variable are distributed).\n",
    "3. Train a model to predict the target variable (risk of `default`).\n",
    "  - Employ three different models (logistic regression, decision tree, and random forest).\n",
    "  - Compare the models' performances (e.g., AUC).\n",
    "  - Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?).\n",
    "4. What more would you do with this data? Anything to help you devise a better solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Load the file, explore the data\n",
    "Load the file from file `data/ccdefault.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originalCC = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 23 more fields]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val originalCC = spark.read.format(\"csv\").option(\"inferSchema\",true).option(\"header\",true).load(\"data/ccdefault.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print schema, the record number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LIMIT_BAL: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- EDUCATION: integer (nullable = true)\n",
      " |-- MARRIAGE: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- PAY_0: integer (nullable = true)\n",
      " |-- PAY_2: integer (nullable = true)\n",
      " |-- PAY_3: integer (nullable = true)\n",
      " |-- PAY_4: integer (nullable = true)\n",
      " |-- PAY_5: integer (nullable = true)\n",
      " |-- PAY_6: integer (nullable = true)\n",
      " |-- BILL_AMT1: integer (nullable = true)\n",
      " |-- BILL_AMT2: integer (nullable = true)\n",
      " |-- BILL_AMT3: integer (nullable = true)\n",
      " |-- BILL_AMT4: integer (nullable = true)\n",
      " |-- BILL_AMT5: integer (nullable = true)\n",
      " |-- BILL_AMT6: integer (nullable = true)\n",
      " |-- PAY_AMT1: integer (nullable = true)\n",
      " |-- PAY_AMT2: integer (nullable = true)\n",
      " |-- PAY_AMT3: integer (nullable = true)\n",
      " |-- PAY_AMT4: integer (nullable = true)\n",
      " |-- PAY_AMT5: integer (nullable = true)\n",
      " |-- PAY_AMT6: integer (nullable = true)\n",
      " |-- DEFAULT: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalCC.printSchema\n",
    "originalCC.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the file so we can read efficiently from memory as opposed to reading it from disk every time we need it.  \n",
    "Show the first 5 rows of data in other to have a glimpse of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|DEFAULT|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|      1|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|      1|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|      0|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|      0|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|      0|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalCC.cache()\n",
    "originalCC.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any NULL value for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|ID: #ofNull|\n",
      "+-----------+\n",
      "|          0|\n",
      "+-----------+\n",
      "\n",
      "+------------------+\n",
      "|LIMIT_BAL: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+------------+\n",
      "|SEX: #ofNull|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n",
      "+------------------+\n",
      "|EDUCATION: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|MARRIAGE: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+------------+\n",
      "|AGE: #ofNull|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n",
      "+--------------+\n",
      "|PAY_0: #ofNull|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|PAY_2: #ofNull|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|PAY_3: #ofNull|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|PAY_4: #ofNull|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|PAY_5: #ofNull|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|PAY_6: #ofNull|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+------------------+\n",
      "|BILL_AMT1: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|BILL_AMT2: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|BILL_AMT3: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|BILL_AMT4: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|BILL_AMT5: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|BILL_AMT6: #ofNull|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|PAY_AMT1: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|PAY_AMT2: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|PAY_AMT3: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|PAY_AMT4: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|PAY_AMT5: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|PAY_AMT6: #ofNull|\n",
      "+-----------------+\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n",
      "+----------------+\n",
      "|DEFAULT: #ofNull|\n",
      "+----------------+\n",
      "|               0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "for (c <- originalCC.columns){ originalCC.select(c).agg(sum(when(col(c).isNotNull,0).otherwise(1)).alias(s\"$c: #ofNull\")).show }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary on `DEFAULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|            DEFAULT|\n",
      "+-------+-------------------+\n",
      "|  count|              30000|\n",
      "|   mean|             0.2212|\n",
      "| stddev|0.41506180569093254|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+\n",
      "|DEFAULT|\n",
      "+-------+\n",
      "|      1|\n",
      "|      0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalCC.describe(\"DEFAULT\").show()\n",
    "originalCC.select(\"DEFAULT\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6636"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalCC.filter(\"DEFAULT = 1\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 data preprocessing & feature engineering\n",
    "From above eplorations, we conclude that all data is numerical(all Int) and there is no NULL.  \n",
    "`featuresCols`: Array containing all features  \n",
    "`catCols`: categorical features (Treat a feature with less than 8 distinct values as categorical.)   \n",
    "`numCols`: numerical features  \n",
    "\n",
    "We will drop the `ID` column as it is irrelevant for prediction `DEFAULT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols = Array(LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = originalCC.columns.filter( _ != \"ID\").filter( _ != \"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catCols = Array(SEX, EDUCATION, MARRIAGE)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SEX, EDUCATION, MARRIAGE]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val catCols = featureCols.filter(originalCC.select(_).distinct().count < 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCols = Array(LIMIT_BAL, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[LIMIT_BAL, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCols = featureCols.filter( _ != \"SEX\").filter( _ != \"EDUCATION\").filter( _ != \"MARRIAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1 Prepare continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column assembling all numerical features into an vector `numericalFeatures`.  \n",
    "And declare a `StandardScaler` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_17b2c36cd818\n",
       "scaler = stdScal_009efd2f1170\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "stdScal_009efd2f1170"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(numCols).setOutputCol(\"numericalFeatures\")\n",
    "val scaler = new StandardScaler().setInputCol(\"numericalFeatures\").setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 Prepare categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have ordinal variables like education and also nominal variables like sex. We will use One-Hot Encoding to convert all categorical variables into binary vectors. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.  \n",
    "`catCols` = Array(SEX, EDUCATION, MARRIAGE)\n",
    "`encodedCatCols` = Array(encodedSEX, encodedEDUCATION, encodedMARRIAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encodedCatCols = Array(encodedSEX, encodedEDUCATION, encodedMARRIAGE)\n",
       "encoding = oneHotEncoder_30e8d55a16c7\n",
       "va2 = vecAssembler_140a2015083c\n",
       "va3 = vecAssembler_08a29474b5ec\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_08a29474b5ec"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val encodedCatCols = Array(\"encodedSEX\", \"encodedEDUCATION\", \"encodedMARRIAGE\")\n",
    "val encoding = new OneHotEncoderEstimator().setInputCols(catCols).setOutputCols(encodedCatCols)\n",
    "//val output2 = encoding.fit(scaledCC).transform(scaledCC)\n",
    "val va2 = new VectorAssembler().setInputCols(encodedCatCols).setOutputCol(\"encodedFeatures\")\n",
    "//val output3 = va2.transform(output2)\n",
    "\n",
    "//Put `scaledFeatures` and `encodedFeatures` into one vector `features`\n",
    "val va3 = new VectorAssembler().setInputCols(Array(\"scaledFeatures\",\"encodedFeatures\")).setOutputCol(\"features\")\n",
    "//val output5 = va3.transform(output4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3 Pipeline\n",
    "Since there are multiple estimators and transformations, we use a Pipeline to tie the stages together. This will simplify our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline = pipeline_542a3ef702bd\n",
       "output = [ID: int, LIMIT_BAL: int ... 30 more fields]\n",
       "processedCC = [ID: int, LIMIT_BAL: int ... 31 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 31 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(va,scaler,encoding,va2,va3))\n",
    "val output = pipeline.fit(originalCC).transform(originalCC)\n",
    "val processedCC = output.withColumn(\"label\",output(\"DEFAULT\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(31,[0,1,2,3,4,5,...|  1.0|\n",
      "|(31,[0,1,2,3,7,8,...|  1.0|\n",
      "|(31,[0,1,8,9,10,1...|  0.0|\n",
      "|(31,[0,1,8,9,10,1...|  0.0|\n",
      "|(31,[0,1,2,4,8,9,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = processedCC.select(\"features\",\"label\")\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31,[0,1,2,3,4,5,6,7,8,9,10,15,24,29],[0.15414535998894324,2.603628744963987,1.7796736791807803,1.6705842242124869,-0.8355143261989371,-0.8553305663148897,-1.7649331341008119,-1.7391491486204211,0.053139869207970765,0.0435834725779124,0.009935199510232218,0.029903384202815683,1.0,1.0])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select(\"features\").head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDataset = [features: vector, label: double]\n",
       "testDataset = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainDataset, testDataset) = dataset.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23863"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Make models\n",
    "Here we going to make three different models:\n",
    "* Logistic regression\n",
    "* Decision tree\n",
    "* Random forest  \n",
    "Before we make any model, we divide the data into `trainSet` and `testSet` with the proportion of 0.8:0.2.\n",
    "\n",
    "The attibutes of dataset used for all three models are following:  \n",
    "* Continuous features: `SEX`, `EDUCATION`, `MARRIAGE`   \n",
    "* Categorical features: `LIMIT_BAL`, `AGE`, `PAY_0`, `PAY_2`, `PAY_3`, `PAY_4`, `PAY_5`, `PAY_6`, `BILL_AMT1`, `BILL_AMT2`, `BILL_AMT3`, `BILL_AMT4`, `BILL_AMT5`, `BILL_AMT6`, `PAY_AMT1`, `PAY_AMT2`, `PAY_AMT3`, `PAY_AMT4`, `PAY_AMT5`, `PAY_AMT6` . \n",
    "* Size of the `features` vector: 31\n",
    "* Number of traing data: 24000\n",
    "* Number of test data: 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 Logistic regression\n",
    "We use `ParamGridBuilder` to tune hyperparameter `maxIter`, `regParam`, `elasticNetParam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7291466302796468\n",
      "accurary: 0.8161968388463419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = logreg_a46f863c7dca\n",
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 10,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 100,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.2,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.5,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_a46f863c7dca-elasticNetParam: 0.8,\n",
       "\tlogreg_a46f863c7dca-maxIter: 1000,\n",
       "\tlogreg_a46f863c7dca-regParam: 0.3\n",
       "}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val lr = new LogisticRegression()//.setMaxIter(100).setRegParam(0.0001).setElasticNetParam(0.8)\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.maxIter,Array(10,100,1000))\n",
    "                .addGrid(lr.regParam,Array(0.001,0.1,0.3))\n",
    "                .addGrid(lr.elasticNetParam,Array(0.2,0.5,0.8))\n",
    "                .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")//we will use Area Under ROC as the evaluation metric. This metric is applicable when one class (label) dominates the other. \n",
    "val cv = new CrossValidator().setEstimator(lr).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainDataset)\n",
    "\n",
    "val Predictions = cvModel.transform(testDataset)\n",
    "val AUC = evaluator.evaluate(Predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "\n",
    "\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val accurary = evaluator2.evaluate(Predictions)\n",
    "println(s\"accurary: $accurary\")\n",
    "\n",
    "//val trainingSummary = lrModel.binarySummary\n",
    "//println(s\"threshold:${lrModel.getThreshold}\")\n",
    "//println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "//println(s\"areaUnderROC: ${trainingSummary.areaUnderROC}\")\n",
    "//println(s\"accuracy: ${trainingSummary.accuracy}\")\n",
    "//println(s\"weightedFMeasure: ${trainingSummary.weightedFMeasure}\")\n",
    "//println(s\"precisionByLabel: ${trainingSummary.precisionByLabel.mkString(\" and \")}\")//se the mkString method to print and specify the separator between precision for each label.\n",
    "//val objectiveHistory = trainingSummary.objectiveHistory\n",
    "//println(\"objectiveHistory:\")\n",
    "//objectiveHistory.foreach(loss => println(loss)) //objective function (scaled loss + regularization) at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Decision tree\n",
    "We use `ParamGridBuilder` to tune hyperparameter `maxDepth`, `impurity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6622839128593611\n",
      "accurary: 0.7471077073488676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt = dtc_1dfc5dd7d417\n",
       "evaluator = binEval_422684dcc23a\n",
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tdtc_1dfc5dd7d417-impurity: entropy,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: gini,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: entropy,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 20\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: gini,\n",
       "\tdt...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\tdtc_1dfc5dd7d417-impurity: entropy,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: gini,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: entropy,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 20\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: gini,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 20\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: entropy,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 30\n",
       "}, {\n",
       "\tdtc_1dfc5dd7d417-impurity: gini,\n",
       "\tdtc_1dfc5dd7d417-maxDepth: 30\n",
       "}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val dt = new DecisionTreeClassifier()//.setMaxDepth(30)\n",
    "//val dtModel = dt.fit(trainDataset)\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "val paramGrid = new ParamGridBuilder().addGrid(dt.maxDepth,Array(10,20,30)).addGrid(dt.impurity,Array(\"entropy\",\"gini\")).build()\n",
    "val cv = new CrossValidator().setEstimator(dt).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainDataset)\n",
    "\n",
    "val Predictions = cvModel.transform(testDataset)\n",
    "val AUC = evaluator.evaluate(Predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val accurary = evaluator2.evaluate(Predictions)\n",
    "println(s\"accurary: $accurary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 Random forest \n",
    "We use `ParamGridBuilder` to tune hyperparameter `maxDepth`, `numTrees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7877902727457997\n",
      "accurary: 0.8259736027374939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf = rfc_af60664d385d\n",
       "evaluator = binEval_b53427780732\n",
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\trfc_af60664d385d-maxDepth: 10,\n",
       "\trfc_af60664d385d-numTrees: 25\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 10,\n",
       "\trfc_af60664d385d-numTrees: 50\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 10,\n",
       "\trfc_af60664d385d-numTrees: 100\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 20,\n",
       "\trfc_af60664d385...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\trfc_af60664d385d-maxDepth: 10,\n",
       "\trfc_af60664d385d-numTrees: 25\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 10,\n",
       "\trfc_af60664d385d-numTrees: 50\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 10,\n",
       "\trfc_af60664d385d-numTrees: 100\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 20,\n",
       "\trfc_af60664d385d-numTrees: 25\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 20,\n",
       "\trfc_af60664d385d-numTrees: 50\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 20,\n",
       "\trfc_af60664d385d-numTrees: 100\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 30,\n",
       "\trfc_af60664d385d-numTrees: 25\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 30,\n",
       "\trfc_af60664d385d-numTrees: 50\n",
       "}, {\n",
       "\trfc_af60664d385d-maxDepth: 30,\n",
       "\trfc_af60664d385d-numTrees: 100\n",
       "}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val rf = new RandomForestClassifier()//.setNumTrees(50).setMaxDepth(30)\n",
    "//val rfModel = rf.fit(trainDataset)\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.maxDepth,Array(10,20,30)).addGrid(rf.numTrees,Array(25,50,100)).build()\n",
    "val cv = new CrossValidator().setEstimator(rf).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainDataset)\n",
    "\n",
    "val Predictions = cvModel.transform(testDataset)\n",
    "val AUC = evaluator.evaluate(Predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val accurary = evaluator2.evaluate(Predictions)\n",
    "println(s\"accurary: $accurary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Improvement\n",
    "#### What more would you do with this data? Anything to help you devise a better solution?\n",
    "- Treating PAY_0 - PAY_6 as categorical features besides SEX, EDUCATION, MARRIAGE\n",
    "- Adding interactions between numerical features\n",
    "    * With degree 2, the total number of features is 330\n",
    "    * With degree 3, the total number of features is 2358 \n",
    "- Adding new features by intepreting features  \n",
    "    * `Total expense in 5 months` = `bill statement in one momth` - `bill statement in previous month` + `payment in previous month`  \n",
    "    * `Total payment in 5 months` = `PAY_AMT1` + `PAY_AMT2` + `PAY_AMT3` + `PAY_AMT4` + `PAY_AMT5`  \n",
    "    * `Repayment ability` = `Total expense in 5 months` / `Total payment in 5 months`  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originalCC = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 23 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val originalCC = spark.read.format(\"csv\").option(\"inferSchema\",true).option(\"header\",true).load(\"data/ccdefault.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addpay = [ID: int, LIMIT_BAL: int ... 23 more fields]\n",
       "addpay2 = [ID: int, LIMIT_BAL: int ... 23 more fields]\n",
       "addpay3 = [ID: int, LIMIT_BAL: int ... 23 more fields]\n",
       "addpay4 = [ID: int, LIMIT_BAL: int ... 23 more fields]\n",
       "addpay5 = [ID: int, LIMIT_BAL: int ... 23 more fields]\n",
       "addpay6 = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 23 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val addpay = originalCC.withColumn(\"PAY_0\",when(col(\"PAY_0\").lt(0),0).otherwise(col(\"PAY_0\")))\n",
    "val addpay2 = addpay.withColumn(\"PAY_2\",when(col(\"PAY_2\").lt(0),0).otherwise(col(\"PAY_2\")))\n",
    "val addpay3 = addpay2.withColumn(\"PAY_3\",when(col(\"PAY_3\").lt(0),0).otherwise(col(\"PAY_3\")))\n",
    "val addpay4 = addpay3.withColumn(\"PAY_4\",when(col(\"PAY_4\").lt(0),0).otherwise(col(\"PAY_4\")))\n",
    "val addpay5 = addpay4.withColumn(\"PAY_5\",when(col(\"PAY_5\").lt(0),0).otherwise(col(\"PAY_5\")))\n",
    "val addpay6 = addpay5.withColumn(\"PAY_6\",when(col(\"PAY_6\").lt(0),0).otherwise(col(\"PAY_6\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit1 = [ID: int, LIMIT_BAL: int ... 24 more fields]\n",
       "credit2 = [ID: int, LIMIT_BAL: int ... 25 more fields]\n",
       "credit3 = [ID: int, LIMIT_BAL: int ... 26 more fields]\n",
       "credit4 = [ID: int, LIMIT_BAL: int ... 27 more fields]\n",
       "credit5 = [ID: int, LIMIT_BAL: int ... 28 more fields]\n",
       "credit6 = [ID: int, LIMIT_BAL: int ... 29 more fields]\n",
       "credit7 = [ID: int, LIMIT_BAL: int ... 30 more fields]\n",
       "creditfinal = [ID: int, LIMIT_BAL: int ... 31 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 31 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//\n",
    "val credit1 = addpay6.withColumn(\"CON_1\", $\"BILL_AMT1\" - $\"BILL_AMT2\" + $\"PAY_AMT1\") \n",
    "val credit2 = credit1.withColumn(\"CON_2\", $\"BILL_AMT2\" - $\"BILL_AMT3\" + $\"PAY_AMT2\") \n",
    "val credit3 = credit2.withColumn(\"CON_3\", $\"BILL_AMT3\" - $\"BILL_AMT4\" + $\"PAY_AMT3\") \n",
    "val credit4 = credit3.withColumn(\"CON_4\", $\"BILL_AMT4\" - $\"BILL_AMT5\" + $\"PAY_AMT4\") \n",
    "val credit5 = credit4.withColumn(\"CON_5\", $\"BILL_AMT5\" - $\"BILL_AMT6\" + $\"PAY_AMT5\") \n",
    "val credit6 = credit5.withColumn(\"TOTAL_CON\", $\"CON_1\" + $\"CON_2\" + $\"CON_3\" + $\"CON_4\" + $\"CON_5\")\n",
    "val credit7 = credit6.withColumn(\"TOTAL_PAY\", $\"PAY_AMT1\" + $\"PAY_AMT2\" + $\"PAY_AMT3\" + $\"PAY_AMT4\" + $\"PAY_AMT5\")\n",
    "val creditfinal = credit7.withColumn(\"REPAYMENT ABILITY\", $\"TOTAL_CON\" / $\"TOTAL_PAY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing & feature engineering\n",
    "From above eplorations, we conclude that all data is numerical(all Int) and there is no NULL.  \n",
    "`featuresCols`: Array containing all features  \n",
    "`catCols`: categorical features (Treat a feature with less than 8 distinct values as categorical.)   \n",
    "`numCols`: numerical features  \n",
    "\n",
    "We will drop the `ID` column as it is irrelevant for prediction `DEFAULT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols = Array(LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, CON_1, CON_2, CON_3, CON_4, CON_5, TOTAL_CON, TOTAL_PAY, REPAYMENT ABILITY)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, CON_1, CON_2, CON_3, CON_4, CON_5, TOTAL_CON, TOTAL_PAY, REPAYMENT ABILITY]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = creditfinal.columns.filter( _ != \"ID\").filter( _ != \"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catCols = Array(SEX, EDUCATION, MARRIAGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SEX, EDUCATION, MARRIAGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val catCols = featureCols.filter(creditfinal.select(_).distinct().count < 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCols = Array(LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, CON_1, CON_2, CON_3, CON_4, CON_5, TOTAL_CON, TOTAL_PAY, REPAYMENT ABILITY)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, CON_1, CON_2, CON_3, CON_4, CON_5, TOTAL_CON, TOTAL_PAY, REPAYMENT ABILITY]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCols = featureCols.filter( _ != \"SEX\").filter( _ != \"EDUCATION\").filter( _ != \"MARRIAGE\").filter( _ != \"PAY_0\").filter( _ != \"PAY_2\").filter( _ != \"PAY_3\").filter( _ != \"PAY_4\").filter( _ != \"PAY_5\").filter( _ != \"PAY_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column assembling all numerical features into an vector `numericalFeatures`.  \n",
    "And declare a `StandardScaler` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imputer = imputer_d574c60d28f7\n",
       "va = vecAssembler_893c43863fe1\n",
       "scaler = stdScal_f35d79b911d0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "stdScal_f35d79b911d0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer().setStrategy(\"median\").setInputCols(Array(\"REPAYMENT ABILITY\")).setOutputCols(Array(\"REPAYMENT ABILITY\"))\n",
    "val va = new VectorAssembler().setInputCols(numCols).setOutputCol(\"numericalFeatures\")\n",
    "val scaler = new StandardScaler().setInputCol(\"numericalFeatures\").setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)\n",
    "\n",
    "//val va2 = new VectorAssembler().setInputCols(catCols).setOutputCol(\"categoricalFeatures\")\n",
    "//val output = va.transform(originalCC)\n",
    "//val output2 = va2.transform(output1)\n",
    "//output2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polyExpansion = poly_73cbb1d8ea8a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "poly_73cbb1d8ea8a"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PolynomialExpansion\n",
    "\n",
    "val polyExpansion = new PolynomialExpansion()\n",
    "  .setInputCol(\"scaledFeatures\")\n",
    "  .setOutputCol(\"polyScaledFeatures\")\n",
    "  .setDegree(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have ordinal variables like education and also nominal variables like sex. We will use One-Hot Encoding to convert all categorical variables into binary vectors. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.  \n",
    "`catCols` = Array(SEX, EDUCATION, MARRIAGE)\n",
    "`encodedCatCols` = Array(encodedSEX, encodedEDUCATION, encodedMARRIAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encodedCatCols = Array(encodedSEX, encodedEDUCATION, encodedMARRIAGE, encodedPAY_0, encodedPAY_2, encodedPAY_3, encodedPAY_4, encodedPAY_5, encodedPAY_6)\n",
       "encoding = oneHotEncoder_232128280b62\n",
       "va2 = vecAssembler_2162bd8a6eeb\n",
       "va3 = vecAssembler_302a96643a06\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_302a96643a06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val encodedCatCols = Array(\"encodedSEX\", \"encodedEDUCATION\", \"encodedMARRIAGE\",\"encodedPAY_0\",\"encodedPAY_2\",\"encodedPAY_3\",\"encodedPAY_4\",\"encodedPAY_5\",\"encodedPAY_6\")\n",
    "val encoding = new OneHotEncoderEstimator().setInputCols(catCols).setOutputCols(encodedCatCols)\n",
    "//val output2 = encoding.fit(scaledCC).transform(scaledCC)\n",
    "val va2 = new VectorAssembler().setInputCols(encodedCatCols).setOutputCol(\"encodedFeatures\")\n",
    "//val output3 = va2.transform(output2)\n",
    "\n",
    "//Put `scaledFeatures` and `encodedFeatures` into one vector `features`\n",
    "val va3 = new VectorAssembler().setInputCols(Array(\"polyScaledFeatures\",\"encodedFeatures\")).setOutputCol(\"features\")\n",
    "//val output5 = va3.transform(output4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Since there are multiple estimators and transformations, we use a Pipeline to tie the stages together. This will simplify our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline = pipeline_dd01b5e1b43f\n",
       "output = [ID: int, LIMIT_BAL: int ... 45 more fields]\n",
       "processedCC = [ID: int, LIMIT_BAL: int ... 46 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 46 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(imputer,va,scaler,polyExpansion,encoding,va2,va3))\n",
    "val output = pipeline.fit(creditfinal).transform(creditfinal)\n",
    "val processedCC = output.withColumn(\"label\",output(\"DEFAULT\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(334,[0,1,2,3,4,5...|  1.0|\n",
      "|[0.92487215993365...|  1.0|\n",
      "|[0.69365411995024...|  0.0|\n",
      "|[0.38536339997235...|  0.0|\n",
      "|[0.38536339997235...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = processedCC.select(\"features\",\"label\")\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(334,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,54,55,56,57,58,59,64,119,120,121,122,123,124,129,134,135,136,137,138,139,140,145,150,151,152,153,154,155,156,157,162,167,168,169,209,210,211,212,213,214,219,224,225,226,229,230,231,232,233,234,235,240,245,246,247,250,251,252,253,254,255,256,257,262,267,268,269,272,273,274,279,284,288,296,302,310,318,326],[0.15414535998894324,0.023760792006120905,2.603628744963987,0.4013372901700343,6.778882641602746,0.053139869207970765,0.008191264268828014,0.13835649097349934,0.0028238456994402394,0.0435834725779124,0.0067181900700905425,0.1134751820092024,0.002316020032419445,0.0018995190819496419,0.009935199510232218,0.001531464905066718,0.025867571031792727,5.279552025288353E-4,4.3301049541029455E-4,9.87081893081185E-5,0.029903384202815683,0.0046094679228307015,0.07785731068215292,0.0015890619254133244,0.001303293325390196,2.970960880861002E-4,8.942123867812065E-4,0.03740481386767184,0.005765778498951692,0.09738824858589797,0.0019876869166765725,0.0016302316794835929,3.716242884184205E-4,0.001118530520119799,0.0013991201004751755,0.13112095414713684,0.02021168667910413,0.34139028528459037,0.006967750353803184,0.005714706509461447,0.001302712839423815,0.003920960268901611,0.0049045548840251875,0.017192704616455563,0.02699567125349276,0.004161257463512808,0.07028670566519174,0.0014345464395919814,0.0011765650977989396,2.6820737981609125E-4,8.072619293061008E-4,0.001009768058469756,0.0035396981726004044,7.287662664266554E-4,0.07065505832524507,0.010891149400584683,0.18395954083271512,0.003754600558285069,0.0030793927970091195,7.019721008684036E-4,0.002112825354972154,0.0026428393054652895,0.009264358662927739,0.0019073807269446728,0.004992137266943783,0.012857708405394594,0.001981956090782411,0.033476699198650435,6.832569429768951E-4,5.60383581701309E-4,1.2774389825198504E-4,3.8448899441428713E-4,4.8094018966858445E-4,0.0016859149942610005,3.471024691853031E-4,9.084621373121488E-4,1.6532066543815477E-4,0.3794585727266274,0.05849177829383658,0.9879692474740547,0.020164378924536243,0.016538122298884742,0.0037699966259070053,0.011347095489296416,0.014193577283331916,0.0497549701152261,0.010243738883647607,0.02681066758801411,0.004878967680046193,0.1439888084157292,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select(\"features\").head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDataset = [features: vector, label: double]\n",
       "testDataset = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainDataset, testDataset) = dataset.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23916"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.1 Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7723273435808704\n",
      "accurary: 0.8176312247644684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = logreg_cf615b854591\n",
       "lrModel = LogisticRegressionModel: uid = logreg_cf615b854591, numClasses = 2, numFeatures = 334\n",
       "predictions = [features: vector, label: double ... 3 more fields]\n",
       "evaluator1 = binEval_d847f712d341\n",
       "evaluator2 = mcEval_97a2b31ae66f\n",
       "AUC = 0.7723273435808704\n",
       "accurary = 0.8176312247644...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8176312247644684"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "//!!!!!!!!!!!!WITHOUT USING CROSSVALIDATION, JUST FOR TEST!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "//import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "//import org.apache.spark.ml.tuning.CrossValidator\n",
    "val lr = new LogisticRegression().setMaxIter(1000).setRegParam(0.001).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(trainDataset)\n",
    "\n",
    "val predictions = lrModel.transform(testDataset)\n",
    "\n",
    "val evaluator1 = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val AUC = evaluator1.evaluate(predictions)\n",
    "val accurary = evaluator2.evaluate(predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "println(s\"accurary: $accurary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.772468516014658\n",
      "accurary: 0.816621803499327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = logreg_5ed8b99d4da5\n",
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.2,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.5,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 10,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 100,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5ed8b99d4da5-elasticNetParam: 0.8,\n",
       "\tlogreg_5ed8b99d4da5-maxIter: 1000,\n",
       "\tlogreg_5ed8b99d4da5-regParam: 0.3\n",
       "}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val lr = new LogisticRegression()//.setMaxIter(100).setRegParam(0.0001).setElasticNetParam(0.8)\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.maxIter,Array(10,100,1000))\n",
    "                .addGrid(lr.regParam,Array(0.001,0.1,0.3))\n",
    "                .addGrid(lr.elasticNetParam,Array(0.2,0.5,0.8))\n",
    "                .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")//we will use Area Under ROC as the evaluation metric. This metric is applicable when one class (label) dominates the other. \n",
    "val cv = new CrossValidator().setEstimator(lr).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainDataset)\n",
    "\n",
    "val Predictions = cvModel.transform(testDataset)\n",
    "val AUC = evaluator.evaluate(Predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val accurary = evaluator2.evaluate(Predictions)\n",
    "println(s\"accurary: $accurary\")\n",
    "\n",
    "//val trainingSummary = lrModel.binarySummary\n",
    "//println(s\"threshold:${lrModel.getThreshold}\")\n",
    "//println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "//println(s\"areaUnderROC: ${trainingSummary.areaUnderROC}\")\n",
    "//println(s\"accuracy: ${trainingSummary.accuracy}\")\n",
    "//println(s\"weightedFMeasure: ${trainingSummary.weightedFMeasure}\")\n",
    "//println(s\"precisionByLabel: ${trainingSummary.precisionByLabel.mkString(\" and \")}\")//se the mkString method to print and specify the separator between precision for each label.\n",
    "//val objectiveHistory = trainingSummary.objectiveHistory\n",
    "//println(\"objectiveHistory:\")\n",
    "//objectiveHistory.foreach(loss => println(loss)) //objective function (scaled loss + regularization) at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6361871188323862\n",
      "accurary: 0.7269515477792732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt = dtc_06faaf0e8b31\n",
       "evaluator = binEval_63c32dcc2722\n",
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tdtc_06faaf0e8b31-impurity: entropy,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: gini,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: entropy,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 20\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: gini,\n",
       "\tdt...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\tdtc_06faaf0e8b31-impurity: entropy,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: gini,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 10\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: entropy,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 20\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: gini,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 20\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: entropy,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 30\n",
       "}, {\n",
       "\tdtc_06faaf0e8b31-impurity: gini,\n",
       "\tdtc_06faaf0e8b31-maxDepth: 30\n",
       "}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val dt = new DecisionTreeClassifier()//.setMaxDepth(30)\n",
    "//val dtModel = dt.fit(trainDataset)\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "val paramGrid = new ParamGridBuilder().addGrid(dt.maxDepth,Array(10,20,30)).addGrid(dt.impurity,Array(\"entropy\",\"gini\")).build()\n",
    "val cv = new CrossValidator().setEstimator(dt).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainDataset)\n",
    "\n",
    "val Predictions = cvModel.transform(testDataset)\n",
    "val AUC = evaluator.evaluate(Predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val accurary = evaluator2.evaluate(Predictions)\n",
    "println(s\"accurary: $accurary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7711840121167566\n",
      "accurary: 0.8088428665351742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf = rfc_bfc24d4b2734\n",
       "evaluator = binEval_bc73c657cb14\n",
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\trfc_bfc24d4b2734-maxDepth: 10,\n",
       "\trfc_bfc24d4b2734-numTrees: 25\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 10,\n",
       "\trfc_bfc24d4b2734-numTrees: 50\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 10,\n",
       "\trfc_bfc24d4b2734-numTrees: 100\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 20,\n",
       "\trfc_bfc24d4b273...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\trfc_bfc24d4b2734-maxDepth: 10,\n",
       "\trfc_bfc24d4b2734-numTrees: 25\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 10,\n",
       "\trfc_bfc24d4b2734-numTrees: 50\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 10,\n",
       "\trfc_bfc24d4b2734-numTrees: 100\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 20,\n",
       "\trfc_bfc24d4b2734-numTrees: 25\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 20,\n",
       "\trfc_bfc24d4b2734-numTrees: 50\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 20,\n",
       "\trfc_bfc24d4b2734-numTrees: 100\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 30,\n",
       "\trfc_bfc24d4b2734-numTrees: 25\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 30,\n",
       "\trfc_bfc24d4b2734-numTrees: 50\n",
       "}, {\n",
       "\trfc_bfc24d4b2734-maxDepth: 30,\n",
       "\trfc_bfc24d4b2734-numTrees: 100\n",
       "}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val rf = new RandomForestClassifier()//.setNumTrees(50).setMaxDepth(30)\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.maxDepth,Array(10,20,30)).addGrid(rf.numTrees,Array(25,50,100)).build()\n",
    "val cv = new CrossValidator().setEstimator(rf).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(3)\n",
    "val cvModel = cv.fit(trainDataset)\n",
    "\n",
    "val Predictions = cvModel.transform(testDataset)\n",
    "val AUC = evaluator.evaluate(Predictions)\n",
    "println(s\"AUC: $AUC\")\n",
    "\n",
    "val evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "val accurary = evaluator2.evaluate(Predictions)\n",
    "println(s\"accurary: $accurary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5  Summary\n",
    "Here is the summary for each model.\n",
    "#### Logistic regression\n",
    "Original logistic regreesion model gives us ability to predict default with 0.726 AUC, 0.816 accuracy.       \n",
    "Improved logistic regreesion model gives us ability to predict default with 0.772 AUC, 0.818 accuracy.  \n",
    "In this dataset, all features are expressed as numerical. However, we regard `SEX`, `EDUCATION`, `MARRIAGE`, `PAY_0`, `PAY_2`, `PAY_3`, `PAY_4`, `PAY_5`, `PAY_6` as categorical values because comparing their numerical values directly are meaningless. As logistic regression expects continuous features, so we used one-hot encoding maps those categorical features, represented as label indexes.\n",
    "\n",
    "#### Decision tree\n",
    "Original Decision tree model gives us ability to predict default with 0.623 AUC, 0.722 accuracy.      \n",
    "Improved Decision tree model gives us ability to predict default with 0.63 AUC, 0.727 accuracy.      \n",
    "Decision trees is a non-linear classifier. It will partition the feature space into half-spaces using axis-aliged linear decision boundaries. Compared to logistic regressions, both continuous and categorical features are handled well with decision tree models. And it is very fast as well. Decision trees are prone to overfitting, especially when the depth of the tree is particularly big. \n",
    "\n",
    "#### Random forest  \n",
    "Original Decision tree model gives us ability to predict default with 0.787 AUC, 0.826 accuracy.          \n",
    "Improved Decision tree model gives us ability to predict default with 0.771 AUC, 0.809 accuracy.       \n",
    "A random forest is simply a collection of decision trees. Random forests mitigate the problem of easy-overfitting with decision trees. Because it is training on different samples of the data with subsets of featues. If we use many trees in our forest, eventually many or all of our features will have been included. This inclusion of many features will help limit the error due to bias and variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
